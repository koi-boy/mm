#!/usr/bin/env python
# encoding:utf-8
"""
author: sunchongjing
@l@icense: (C) Copyright 2019, Union Big Data Co. Ltd. All rights reserved.
@contact: sunchongjing@unionbigdata.com
@software:
@file: test.py
@time: 2019/9/6 16:37
@desc:
"""
import os
import os.path as osp
import shutil
import tempfile

import mmcv
import torch
import torch.distributed as dist
from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
from mmcv.runner import get_dist_info, load_checkpoint

from mmdet.apis import init_dist
from mmdet.core import coco_eval, results2json, wrap_fp16_model
from mmdet.datasets import build_dataloader, build_dataset
from mmdet.models import build_detector


class TestMMDetModel(object):

    def __init__(self, config, checkpoint, out_pkl, show_result=False, out_json=None,
                 tmpdir=None, launcher='none', eval_types=None):
        """

        :param config: test config file path, can use the training config file
        :param checkpoint: checkpoint file, generated by training process
        :param out_pkl: output result file
        :param show_result: show results
        :param out_json: Save predictions in the COCO json format
        :param tmpdir: tmp dir for writing some results
        :param launcher: job launcher in ['none', 'pytorch', 'slurm', 'mpi']
        :param eval_types: nargs='+', ['proposal', 'proposal_fast', 'bbox', 'segm', 'keypoints']
        """

        # args = parse_args()

        assert out_pkl or show_result or out_json, \
            ('Please specify at least one operation (save or show the results) '
             'with the argument "--out" or "--show" or "--json_out"')
        self.show = show_result
        self.out_pkl = out_pkl
        self.out_json = out_json

        if out_pkl is not None and not out_pkl.endswith(('.pkl', '.pickle')):
            raise ValueError('The output file must be a pkl file.')

        if out_json is not None and out_json.endswith('.json'):
            out_json = out_json[:-5]

        cfg = mmcv.Config.fromfile(config)
        # set cudnn_benchmark
        if cfg.get('cudnn_benchmark', False):
            torch.backends.cudnn.benchmark = True
        cfg.model.pretrained = None
        cfg.data.test.test_mode = True

        # init distributed env first, since logger depends on the dist info.
        if launcher == 'none':
            self.distributed = False
        else:
            self.distributed = True
            init_dist(launcher, **cfg.dist_params)
        self.tmpdir = tmpdir
        self.eval_types = eval_types

        # build the dataloader
        # TODO: support multiple images per gpu (only minor changes are needed)
        dataset = build_dataset(cfg.data.test)
        self.dataset = dataset
        self.data_loader = build_dataloader(
            dataset,
            imgs_per_gpu=1,
            workers_per_gpu=cfg.data.workers_per_gpu,
            dist=self.distributed,
            shuffle=False)

        # build the model and load checkpoint
        self.model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)
        fp16_cfg = cfg.get('fp16', None)
        if fp16_cfg is not None:
            wrap_fp16_model(self.model)
        self.checkpoint = load_checkpoint(self.model, checkpoint, map_location='cpu')
        # old versions did not save class info in checkpoints, this walkaround is
        # for backward compatibility
        if 'CLASSES' in self.checkpoint['meta']:
            self.model.CLASSES = self.checkpoint['meta']['CLASSES']
        else:
            self.model.CLASSES = dataset.CLASSES

    def start_testing(self):

        if not self.distributed:
            self.model = MMDataParallel(self.model, device_ids=[0])
            outputs = self.__single_gpu_test()
        else:
            self.model = MMDistributedDataParallel(self.model.cuda())
            outputs = self.__multi_gpu_test()

        dir_name = os.path.dirname(self.out_pkl)
        if not os.path.exists(dir_name):
            os.makedirs(dir_name)

        rank, _ = get_dist_info()
        if self.out_pkl and rank == 0:
            print('\nwriting results to {}'.format(self.out_pkl))
            mmcv.dump(outputs, self.out_pkl)
            eval_types = self.eval_types
            if eval_types:
                print('Starting evaluate {}'.format(' and '.join(eval_types)))
                if eval_types == ['proposal_fast']:
                    result_file = self.out_pkl
                    coco_eval(result_file, eval_types, self.dataset.coco)
                else:
                    if not isinstance(outputs[0], dict):
                        result_files = results2json(self.dataset, outputs, self.out_pkl)
                        coco_eval(result_files, eval_types, self.dataset.coco)
                    else:
                        for name in outputs[0]:
                            print('\nEvaluating {}'.format(name))
                            outputs_ = [out[name] for out in outputs]
                            result_file = self.out_pkl + '.{}'.format(name)
                            result_files = results2json(self.dataset, outputs_,
                                                        result_file)
                            coco_eval(result_files, eval_types, self.dataset.coco)

        # Save predictions in the COCO json format
        if self.out_json and rank == 0:
            if not isinstance(outputs[0], dict):
                results2json(self.dataset, outputs, self.out_json)
            else:
                for name in outputs[0]:
                    outputs_ = [out[name] for out in outputs]
                    result_file = self.out_json + '.{}'.format(name)
                    results2json(self.dataset, outputs_, result_file)

    def __single_gpu_test(self):
        self.model.eval()
        results = []
        dataset = self.data_loader.dataset
        prog_bar = mmcv.ProgressBar(len(dataset))
        for i, data in enumerate(self.data_loader):
            with torch.no_grad():
                result = self.model(return_loss=False, rescale=not self.show, **data)
            results.append(result)

            if self.show:
                self.model.module.show_result(data, result, dataset.img_norm_cfg)

            batch_size = data['img'][0].size(0)
            for _ in range(batch_size):
                prog_bar.update()
        return results

    def __multi_gpu_test(self):
        self.model.eval()
        results = []
        dataset = self.data_loader.dataset
        rank, world_size = get_dist_info()
        if rank == 0:
            prog_bar = mmcv.ProgressBar(len(dataset))
        for i, data in enumerate(self.data_loader):
            with torch.no_grad():
                result = self.model(return_loss=False, rescale=True, **data)
            results.append(result)

            if rank == 0:
                batch_size = data['img'][0].size(0)
                for _ in range(batch_size * world_size):
                    prog_bar.update()

        # collect results from all ranks
        results = self.__collect_results(results, len(dataset), self.tmpdir)

        return results

    @staticmethod
    def __collect_results(result_part, size, tmpdir=None):
        rank, world_size = get_dist_info()
        # create a tmp dir if it is not specified
        if tmpdir is None:
            MAX_LEN = 512
            # 32 is whitespace
            dir_tensor = torch.full((MAX_LEN, ),
                                    32,
                                    dtype=torch.uint8,
                                    device='cuda')
            if rank == 0:
                tmpdir = tempfile.mkdtemp()
                tmpdir = torch.tensor(
                    bytearray(tmpdir.encode()), dtype=torch.uint8, device='cuda')
                dir_tensor[:len(tmpdir)] = tmpdir
            dist.broadcast(dir_tensor, 0)
            tmpdir = dir_tensor.cpu().numpy().tobytes().decode().rstrip()
        else:
            mmcv.mkdir_or_exist(tmpdir)
        # dump the part result to the dir
        mmcv.dump(result_part, osp.join(tmpdir, 'part_{}.pkl'.format(rank)))
        dist.barrier()
        # collect all parts
        if rank != 0:
            return None
        else:
            # load results of all parts from tmp dir
            part_list = []
            for i in range(world_size):
                part_file = osp.join(tmpdir, 'part_{}.pkl'.format(i))
                part_list.append(mmcv.load(part_file))
            # sort the results
            ordered_results = []
            for res in zip(*part_list):
                ordered_results.extend(list(res))
            # the dataloader may pad some samples
            ordered_results = ordered_results[:size]
            # remove tmp dir
            shutil.rmtree(tmpdir)
            return ordered_results


if __name__ == '__main__':

    # config_ = '/home/scj/mm_detection_proj/stations/visionox_v3/training_models/20190826/faster_rcnn_dconv_c3-c5_x101_32x4d_fpn_1x.py'
    # checkpoint_ = '/home/scj/mm_detection_proj/stations/visionox_v3/training_models/20190826/epoch_24.pth'
    # out_pkl_ = '/home/scj/mm_detection_proj/stations/visionox_v3/test_results/20190826/epoch_24.pkl'
    # testMM = TestMMDetModel(config_, checkpoint_, out_pkl_)
    # testMM.start_testing()

    config_ = '/home/scj/mm_detection_proj/stations/visionox_v3/work_dir/20190911_154938.py'
    checkpoint_ = '/home/scj/mm_detection_proj/stations/visionox_v3/training_models/20190911/epoch_2.pth'
    out_pkl_ = '/home/scj/mm_detection_proj/stations/visionox_v3/work_dir/out_dir/epoch_2_2.pkl'
    testMM = TestMMDetModel(config_, checkpoint_, out_pkl_)
    testMM.start_testing()
